import os, time, json, shutil
from pathlib import Path
from typing import List, Tuple

import streamlit as st
from dotenv import load_dotenv

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import (
    PyPDFLoader, Docx2txtLoader, TextLoader
)
from langchain_community.vectorstores import FAISS

# ====== åˆæœŸè¨­å®š ======
load_dotenv()
ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data_raw"
OUT_DIR = ROOT / "storage"
LOG_DIR = ROOT / "logs"
OUT_DIR.mkdir(exist_ok=True, parents=True)
DATA_DIR.mkdir(exist_ok=True, parents=True)
LOG_DIR.mkdir(exist_ok=True, parents=True)

st.set_page_config(page_title="RAG Indexer & Search", page_icon="ğŸ“š", layout="wide")

# ====== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
def write_log(event: str, payload: dict):
    payload = {"event": event, **payload, "ts": int(time.time())}
    with open(LOG_DIR / "run.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(payload, ensure_ascii=False) + "\n")

def load_docs_from_dir(dirpath: Path):
    docs = []
    for p in dirpath.glob("**/*"):
        if p.is_dir(): 
            continue
        suf = p.suffix.lower()
        if suf == ".pdf":
            docs += PyPDFLoader(str(p)).load()
        elif suf == ".docx":
            docs += Docx2txtLoader(str(p)).load()
        elif suf in [".txt", ".md"]:
            docs += TextLoader(str(p), encoding="utf-8").load()
    return docs

# ====== ã‚¹ãƒ—ãƒªãƒƒãƒˆï¼ˆè³¢ãåˆ‡ã‚‹ï¼‰ ======
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain.schema import Document

def split_documents_smart(raw_docs, chunk_size: int, overlap: int):
    """DOC_IDã‚„Markdownè¦‹å‡ºã—ã‚’å¢ƒç•Œã«ã—ã¦ã‹ã‚‰ã€æœ€å¾Œã«å†å¸°ã‚¹ãƒ—ãƒªãƒƒã‚¿ã§æ•´ãˆã‚‹"""
    chunks = []
    md_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=[("#","h1"),("##","h2"),("###","h3")]
    )
    for d in raw_docs:
        text = d.page_content
        parts = re.split(r"\n(?=DOC_ID:\s*)", text) if "DOC_ID:" in text else [text]
        for part in parts:
            # Markdownè¦‹å‡ºã—ã§åˆ‡ã‚Œã‚‹ãªã‚‰å„ªå…ˆ
            try:
                md_docs = md_splitter.split_text(part)
                for m in md_docs:
                    m.metadata = {**d.metadata, **m.metadata}  # å…ƒã®metadataï¼ˆsource, page ç­‰ï¼‰ã‚’ä¸Šæ›¸ããƒãƒ¼ã‚¸
                docs_lvl = md_docs if md_docs else [Document(page_content=part, metadata=d.metadata)]
            except Exception:
                docs_lvl = [Document(page_content=part, metadata=d.metadata)]
            # æœ€å¾Œã«é•·ã•èª¿æ•´
            rc = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size, chunk_overlap=overlap,
                separators=["\n\n","\n","ã€‚","ã€"," ",""]
            )
            chunks.extend(rc.split_documents(docs_lvl))
    return chunks

def build_index(chunk_size: int, overlap: int) -> Tuple[int, int, int]:
    t0 = time.time()
    raw_docs = load_docs_from_dir(DATA_DIR)
    if not raw_docs:
        return 0, 0, 0
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
    )
    docs = splitter.split_documents(raw_docs)
    embeddings = OpenAIEmbeddings()
    vs = FAISS.from_documents(docs, embeddings)
    vs.save_local(str(OUT_DIR / "faiss_index"))
    dt_ms = int((time.time() - t0) * 1000)
    write_log("ui_ingest", {
        "num_files": len([p for p in DATA_DIR.glob("**/*") if p.is_file()]),
        "num_chunks": len(docs),
        "latency_ms": dt_ms,
        "chunk_size": chunk_size,
        "overlap": overlap
    })
    return len(raw_docs), len(docs), dt_ms

def load_index() -> FAISS:
    embeddings = OpenAIEmbeddings()
    return FAISS.load_local(
        str(OUT_DIR / "faiss_index"),
        embeddings,
        allow_dangerous_deserialization=True
    )

def do_search(vs: FAISS, query: str, k: int, use_mmr: bool):
    if use_mmr:
        docs = vs.max_marginal_relevance_search(query, k=k, fetch_k=max(10, k*3))
        scores = [None] * len(docs)
        return list(zip(docs, scores))
    else:
        return vs.similarity_search_with_score(query, k=k)

def move_uploaded_files(uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile], dest: Path):
    dest.mkdir(exist_ok=True, parents=True)
    saved = []
    for uf in uploaded_files:
        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        if not any(uf.name.lower().endswith(ext) for ext in (".pdf", ".docx", ".txt", ".md")):
            continue
        outpath = dest / uf.name
        with open(outpath, "wb") as f:
            shutil.copyfileobj(uf, f)
        saved.append(outpath)
    return saved

# ====== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ======
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    # â˜…è¿½åŠ ï¼šæ¤œç´¢/ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å¯¾è±¡ã‚¹ã‚³ãƒ¼ãƒ—
    scope = st.selectbox("æ¤œç´¢ã‚¹ã‚³ãƒ¼ãƒ—", ["å…¨éƒ¨", "req", "manuals", "misc"], index=0)
    chunk_size = st.slider("Chunk size", 300, 2000, 800, 50)
    overlap = st.slider("Chunk overlap", 0, 400, 150, 10)
    k = st.slider("Top-K", 1, 20, 5, 1)
    use_mmr = st.toggle("MMRï¼ˆé‡è¤‡æŠ‘åˆ¶ï¼‰", value=False)
    gen_answer = st.toggle("å›ç­”æ–‡ã‚’ç”Ÿæˆï¼ˆè¦APIï¼‰", value=False,
        help="ä¸Šä½ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æ ¹æ‹ ä»˜ãã®çŸ­ã„å›ç­”æ–‡ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    st.caption("â€» ã¾ãšã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰â†’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰â†’æ¤œç´¢ã€‚")

# ====== ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆscopeå¯¾å¿œ & ã‚¹ãƒãƒ¼ãƒˆåˆ†å‰²ï¼‰ ======
def build_index(chunk_size: int, overlap: int, scope: str) -> tuple[int,int,int,str]:
    t0 = time.time()
    target_dir = DATA_DIR if scope == "å…¨éƒ¨" else DATA_DIR / scope
    raw_docs = load_docs_from_dir(target_dir)
    if not raw_docs:
        return 0, 0, 0, target_dir.as_posix()

    docs = split_documents_smart(raw_docs, chunk_size, overlap)

    embeddings = OpenAIEmbeddings()
    vs = FAISS.from_documents(docs, embeddings)

    index_name = "faiss_all" if scope == "å…¨éƒ¨" else f"faiss_{scope}"
    vs.save_local(str(OUT_DIR / index_name))

    dt_ms = int((time.time() - t0) * 1000)
    write_log("ui_ingest", {
        "scope": scope,
        "dir": target_dir.as_posix(),
        "num_files": len([p for p in target_dir.glob("**/*") if p.is_file()]),
        "num_chunks": len(docs),
        "latency_ms": dt_ms,
        "chunk_size": chunk_size,
        "overlap": overlap
    })
    return len(raw_docs), len(docs), dt_ms, target_dir.as_posix()

def load_index(scope: str) -> FAISS:
    embeddings = OpenAIEmbeddings()
    index_name = "faiss_all" if scope == "å…¨éƒ¨" else f"faiss_{scope}"
    return FAISS.load_local(
        str(OUT_DIR / index_name),
        embeddings,
        allow_dangerous_deserialization=True
    )

# ====== ãƒ¡ã‚¤ãƒ³ï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ======
st.subheader("1) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
uploaded = st.file_uploader(
    f"{' â†’ data_raw/' + scope if scope != 'å…¨éƒ¨' else 'ï¼ˆâ€»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã¯ã‚¹ã‚³ãƒ¼ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼‰'}",
    type=["pdf","docx","txt","md"],
    accept_multiple_files=True
)

cols = st.columns(2)
if uploaded:
    upload_dir = DATA_DIR if scope == "å…¨éƒ¨" else DATA_DIR / scope
    saved_paths = move_uploaded_files(uploaded, upload_dir)
    with cols[0]:
        st.success(f"ä¿å­˜: {len(saved_paths)} ä»¶ â†’ {upload_dir.as_posix()}")
    with cols[1]:
        st.write("\n".join([p.name for p in saved_paths]) or "-")

reindex = st.button("ğŸ“¦ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰ã™ã‚‹ï¼ˆingestï¼‰")
if reindex:
    with st.status(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­â€¦ï¼ˆscope={scope}ï¼‰", expanded=True) as status:
        n_files, n_chunks, dt_ms, where = build_index(chunk_size, overlap, scope)
        if n_files == 0:
            st.warning("å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            status.update(label="æ§‹ç¯‰å¤±æ•—", state="error")
        else:
            st.write(f"Scope: {scope}  /  Path: {where}")
            st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {n_files} / ãƒãƒ£ãƒ³ã‚¯æ•°: {n_chunks} / æ™‚é–“: {dt_ms} ms")
            status.update(label="æ§‹ç¯‰å®Œäº†", state="complete")

st.divider()

# ====== ãƒ¡ã‚¤ãƒ³ï¼šæ¤œç´¢ ======
st.subheader("2) æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ«è¿‘å‚ï¼‰")
query = st.text_input("è³ªå•ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›", value="ç¤¾å†…æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦æ•™ãˆã¦")
col1, col2 = st.columns([1,1])
search_clicked = col1.button("ğŸ” æ¤œç´¢ã™ã‚‹")
hint = col2.caption("â€» ä¾‹ï¼šã€ç´æœŸã¯ï¼Ÿã€ã€ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒ•ãƒ­ãƒ¼ã€ãªã©è³‡æ–™ã«å«ã¾ã‚Œã‚‹èªã§ã€‚")

if search_clicked and query.strip():
    try:
        vs = load_index(scope)
    except Exception as e:
        st.error("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰ã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.exception(e)
    else:
        t0 = time.time()
        results = do_search(vs, query.strip(), k=k, use_mmr=use_mmr)
        dt_ms = int((time.time() - t0) * 1000)
        write_log("ui_query", {"scope": scope, "query": query, "k": k, "mmr": use_mmr, "latency_ms": dt_ms})

        st.caption(f"æ¤œç´¢æ™‚é–“: {dt_ms} ms / Top-K: {k} / {'MMR: ON' if use_mmr else 'MMR: OFF'}")

        if not results:
            st.warning("è©²å½“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã‚‹ã‹ã€è³‡æ–™ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        else:
            for idx, item in enumerate(results, start=1):
                if use_mmr:
                    doc, score = item[0], None
                else:
                    doc, score = item

                src = Path(doc.metadata.get("source", "unknown"))
                page = doc.metadata.get("page")
                loc = f"{src.name}" + (f"  p.{page+1}" if isinstance(page, int) else "")
                snippet = doc.page_content[:400].replace("\n", " ")

                with st.container(border=True):
                    st.markdown(f"**[{idx}] {loc}**" + (f"  _(distance: {score:.4f})_" if score is not None else ""))
                    st.write(snippet + " â€¦")
                    with st.expander("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"):
                        st.json(doc.metadata)

            # ====== å›ç­”ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ ======
            if gen_answer:
                st.divider()
                st.subheader("ğŸ“ å›ç­”ï¼ˆä¸Šä½æ–‡æ›¸ã«åŸºã¥ãçŸ­ã„è¦ç´„ï¼‰")
                try:
                    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
                    # çŸ­ã„æ ¹æ‹ ä»˜ãã®è¦ç´„ï¼ˆå¹»è¦šæŠ‘åˆ¶ã®ãŸã‚ã€å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ææ–™ã«ï¼‰
                    context = "\n\n".join([d[0].page_content if not use_mmr else d.page_content for d in results[:k]])
                    prompt = (
                        "ã‚ãªãŸã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQAã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç¯„å›²å†…ã§ã€"
                        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚å‡ºå…¸ã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ç« ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°æ–‡æœ«ã«æ‹¬å¼§ã§ä»˜ã‘ã¦ãã ã•ã„ã€‚"
                        "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã®æ¨æ¸¬ã¯é¿ã‘ã€ã€ä¸æ˜ã€ã¨è¿°ã¹ã¦ãã ã•ã„ã€‚\n\n"
                        f"# è³ªå•:\n{query}\n\n# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}"
                    )
                    ans = llm.invoke(prompt).content
                    st.success(ans)
                except Exception as e:
                    st.warning("å›ç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIã‚­ãƒ¼ã‚„æ¨©é™ã‚’ã”ç¢ºèªãã ã•ã„ï¼‰ã€‚")
                    st.exception(e)

# ====== ãƒ•ãƒƒã‚¿ãƒ¼ ======
st.divider()
with st.expander("ğŸ§ª ä½¿ã„æ–¹ãƒ’ãƒ³ãƒˆ", expanded=False):
    st.markdown("""
- **æ‰‹é †**ï¼šâ‘ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ â‘¡ã€Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰ã€ â†’ â‘¢æ¤œç´¢  
- **MMR**ï¼šä¼¼ãŸå†…å®¹ã®é‡è¤‡ã‚’æ¸›ã‚‰ã—ã€å¤šæ§˜æ€§ã‚’ä¸Šã’ã¾ã™ï¼ˆè·é›¢ã‚¹ã‚³ã‚¢ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ï¼‰ã€‚  
- **Chunkè¨­è¨ˆ**ï¼šè¦ç´„ä¸­å¿ƒã¯çŸ­ã‚ï¼ˆ500/100ï¼‰ã€ä»•æ§˜æ›¸ä¸­å¿ƒã¯é•·ã‚ï¼ˆ1000/200ï¼‰ã‚’è©¦ã™ã®ãŒãŠã™ã™ã‚ã€‚  
- **PDFã®æ³¨æ„**ï¼šç”»åƒã ã‘ã®PDFã¯ãƒ†ã‚­ã‚¹ãƒˆãŒå–ã‚Œã¾ã›ã‚“ï¼ˆOCRã¯åˆ¥é€”ï¼‰ã€‚  
- **ä¿å­˜å ´æ‰€**ï¼š`data_raw/`ï¼ˆåŸæ–‡æ›¸ï¼‰ã€`storage/faiss_index/`ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã€`logs/run.jsonl`ï¼ˆãƒ­ã‚°ï¼‰
""")